{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import h5py\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from image import *\n",
    "from model import CDENet\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CDENet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform=transforms.Compose([\n",
    "                      transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now generate the ShanghaiA's ground truth\n",
    "part_A_train = os.path.join(root,'part_A_final/train_data','images')\n",
    "part_A_test = os.path.join(root,'part_A_final/test_data','images')\n",
    "part_B_train = os.path.join(root,'part_B_final/train_data','images')\n",
    "part_B_test = os.path.join(root,'part_B_final/test_data','images')\n",
    "path_sets = [part_A_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining the image path\n",
    "img_paths = []\n",
    "for path in path_sets:\n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "       img_paths.append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('0model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae = 0\n",
    "for i in tqdm(range(len(img_paths))):\n",
    "    img = transform(Image.open(img_paths[i]).convert('RGB')).cuda()\n",
    "    gt_file = h5py.File(img_paths[i].replace('.jpg','.h5').replace('images','ground_truth'),'r')\n",
    "    groundtruth = np.asarray(gt_file['density'])\n",
    "    output = model(img.unsqueeze(0))\n",
    "    mae += abs(output.detach().cpu().sum().numpy()-np.sum(groundtruth))\n",
    "print (mae/len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm as c\n",
    "img = transform(Image.open('dataset/part_A_final/test_data/images/IMG_87.jpg').convert('RGB')).cuda()\n",
    "\n",
    "output = model(img.unsqueeze(0))\n",
    "print(\"Predicted Count : \",int(output.detach().cpu().sum().numpy()))\n",
    "temp = np.asarray(output.detach().cpu().reshape(output.detach().cpu().shape[2],output.detach().cpu().shape[3]))\n",
    "plt.imshow(temp,cmap = c.jet)\n",
    "plt.show()\n",
    "temp = h5py.File('dataset/part_A_final/test_data/ground_truth/IMG_87.h5', 'r')\n",
    "temp_1 = np.asarray(temp['density'])\n",
    "plt.imshow(temp_1,cmap = c.jet)\n",
    "print(\"Original Count : \",int(np.sum(temp_1)) + 1)\n",
    "plt.show()\n",
    "print(\"Original Image\")\n",
    "plt.imshow(plt.imread('dataset/part_A_final/test_data/images/IMG_87.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm as c\n",
    "img = transform(Image.open('images.jpg').convert('RGB')).cuda()\n",
    "\n",
    "output = model(img.unsqueeze(0))\n",
    "print(\"Predicted Count : \",int(output.detach().cpu().sum().numpy()))\n",
    "temp = np.asarray(output.detach().cpu().reshape(output.detach().cpu().shape[2],output.detach().cpu().shape[3]))\n",
    "plt.imshow(temp,cmap = c.jet)\n",
    "plt.show()\n",
    "\n",
    "print(\"Original Image\")\n",
    "plt.imshow(plt.imread('images.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input frame\n",
    "def preprocess(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (224, 224))  # Resize to match model input size\n",
    "    frame = transform(frame)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(freme):\n",
    "\n",
    "    img = preprocess(freme)\n",
    "    output = model(img.unsqueeze(0))\n",
    "    print(\"Predicted Count : \",int(output.detach().cpu().sum().numpy()))\n",
    "    temp = np.asarray(output.detach().cpu().reshape(output.detach().cpu().shape[2],output.detach().cpu().shape[3]))\n",
    "    plt.imshow(temp,cmap = c.jet)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    predict(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize the camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to a tensor and run the model\n",
    "    img = transform(frame).cuda()\n",
    "    output = model(img.unsqueeze(0))\n",
    "\n",
    "    print(\"Predicted Count : \",int(output.detach().cpu().sum().numpy()))\n",
    "\n",
    "    # Convert the output tensor to a numpy array and reshape it\n",
    "    temp = np.asarray(output.detach().cpu().reshape(output.detach().cpu().shape[2],output.detach().cpu().shape[3]))\n",
    "\n",
    "    # Normalize the values in the array to the range [0, 1]\n",
    "    temp = (temp - temp.min()) / (temp.max() - temp.min())\n",
    "\n",
    "    # Display the processed frame\n",
    "    cv2.imshow(\"Output\", frame)\n",
    "\n",
    "    # Wait for a key press and exit if the 'q' key is pressed\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == ord('q'):\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
